{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precipitaion Forecasting\n",
    "In this project, we have trained different types of machine learning models on some data about weather to predict precipitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "_Weather forecasting_ is using data about the current state and predict how the atmosphere will change. Weather warnings are used to protect lives and property, weather forecasting improves transportation safety, and precipitation forecasting is important to agriculture. There are many different ways of weather prediction. We have used machine learning models and compared the predicted results with actual values.\n",
    "#### Study area\n",
    "Basel is a city in northwest Switzerland. On average $51\\%$ days of the year have precipitation more than $0.1mm$. The total precipitation is around $850 mm$ annually.\n",
    "#### Methodology\n",
    "At first, data is collected. Then, some preprocessing techniques are used to prepare data for machine learning models. Finally, different machine learning techniquies are applied and the accuracy for each is reported.\\\n",
    "The following libraries are used: `numpy`, `pandas`, `matplotlib`, `scikit-learn`, `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import  make_pipeline\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "from getdata import load_data, write_daily_data, read_daily_data\n",
    "from preprocessing import drop_missing_data, change_resolution_to_daily\n",
    "from visualize import show_histogram\n",
    "# %matplotlib widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "**You can find and download the dataset in [this](https://www.meteoblue.com/en/weather/archive/export) link.**\n",
    "#### About dataset\n",
    "This dataset contains some attributes about weather for Basel, from *January, 2014* to *November, 2023* with hourly resolution. The first nine rows are some basic information about location of city and units of measurements which we do not need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86664, 7)\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the initial stage, there are $86664$ _samples_ and $7$ parameters.\n",
    "#### Cleaning dataset\n",
    "There are some rows at the end of dataset which are empty, _missing data_. We simply drop them. A day after the missing data is not complete. For simplicity we remove this day as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "missing_data = drop_missing_data(data)\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, number of missing rows is $191$.\n",
    "#### Parameters\n",
    "In our data, each row represents a sample and each column represents a feature. Here is the list of columns measured hourly. This list is raw and we will do some operations to get ready for models.\n",
    "> Level column gives the height of the measurment from the sea level.\n",
    "\n",
    "| Symbol        | Parameter                 | Level                     | Unit          |\n",
    "|:-------------:|:-------------------------:|:-------------------------:|:-------------:|\n",
    "| $\\text{T}$    | Temperature               | $2 m$ elevation corrected | $°C$          |\n",
    "| $\\text{PT}$   | Precipitation Total       | sfc                       | $mm$          |\n",
    "| $\\text{RH}$   | Relative Humidity         | $2 m$                     | $\\%$          |\n",
    "| $\\text{WS}$   | Wind Speed                | $100 m$                   | $km/h$        |\n",
    "| $\\text{WD}$   | Wind Direction            | $100 m$                   | $°$           |\n",
    "| $\\text{CCT}$  | Cloud Cover Total         | sfc                       | $\\%$          |\n",
    "| $\\text{MSL}$  | Mean Sea Level Pressure   | MSL                       | $\\text{hPa}$  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "#### Make samples daily\n",
    "Forecasting for a whole day is more general than one hour, so we decided to merge each $24$ sample to convert the resolution to daily.\n",
    "- We extract **maximum**, **minimum**, and **mean** from each day of all parameters.\n",
    "- Except **Precipitation** which should be the **sum**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PT</th>\n",
       "      <th>TMEAN</th>\n",
       "      <th>RHMEAN</th>\n",
       "      <th>WSMEAN</th>\n",
       "      <th>WDMEAN</th>\n",
       "      <th>CCTMEAN</th>\n",
       "      <th>MSLMEAN</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>RHMAX</th>\n",
       "      <th>WSMAX</th>\n",
       "      <th>WDMAX</th>\n",
       "      <th>CCTMAX</th>\n",
       "      <th>MSLMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>RHMIN</th>\n",
       "      <th>WSMIN</th>\n",
       "      <th>WDMIN</th>\n",
       "      <th>CCTMIN</th>\n",
       "      <th>MSLMIN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2014</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>4.494412</td>\n",
       "      <td>90.203694</td>\n",
       "      <td>16.615975</td>\n",
       "      <td>166.495278</td>\n",
       "      <td>59.916667</td>\n",
       "      <td>1014.029167</td>\n",
       "      <td>8.720245</td>\n",
       "      <td>96.163430</td>\n",
       "      <td>18.455048</td>\n",
       "      <td>202.01129</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>0.310245</td>\n",
       "      <td>77.926410</td>\n",
       "      <td>13.584932</td>\n",
       "      <td>144.46231</td>\n",
       "      <td>30.000002</td>\n",
       "      <td>1009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8</td>\n",
       "      <td>5.978995</td>\n",
       "      <td>92.044333</td>\n",
       "      <td>20.621631</td>\n",
       "      <td>184.099628</td>\n",
       "      <td>63.875000</td>\n",
       "      <td>1008.670833</td>\n",
       "      <td>9.240245</td>\n",
       "      <td>97.294250</td>\n",
       "      <td>29.215502</td>\n",
       "      <td>223.78113</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1012.1</td>\n",
       "      <td>3.530245</td>\n",
       "      <td>83.382390</td>\n",
       "      <td>14.759999</td>\n",
       "      <td>152.35402</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1005.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>6.586079</td>\n",
       "      <td>88.778159</td>\n",
       "      <td>22.263927</td>\n",
       "      <td>183.268500</td>\n",
       "      <td>50.108334</td>\n",
       "      <td>1013.070833</td>\n",
       "      <td>10.400246</td>\n",
       "      <td>97.533720</td>\n",
       "      <td>29.142216</td>\n",
       "      <td>210.57922</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1016.1</td>\n",
       "      <td>2.340245</td>\n",
       "      <td>79.372375</td>\n",
       "      <td>15.716793</td>\n",
       "      <td>157.79652</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1011.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.5</td>\n",
       "      <td>6.358579</td>\n",
       "      <td>94.172092</td>\n",
       "      <td>15.272616</td>\n",
       "      <td>158.277724</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>1007.625000</td>\n",
       "      <td>8.480246</td>\n",
       "      <td>98.024340</td>\n",
       "      <td>28.100763</td>\n",
       "      <td>205.51387</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1016.3</td>\n",
       "      <td>4.600245</td>\n",
       "      <td>88.295070</td>\n",
       "      <td>7.244860</td>\n",
       "      <td>135.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>999.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.995662</td>\n",
       "      <td>86.622807</td>\n",
       "      <td>16.897822</td>\n",
       "      <td>225.090521</td>\n",
       "      <td>59.383333</td>\n",
       "      <td>1010.754167</td>\n",
       "      <td>7.480245</td>\n",
       "      <td>97.288110</td>\n",
       "      <td>27.103000</td>\n",
       "      <td>253.00919</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1016.2</td>\n",
       "      <td>0.440245</td>\n",
       "      <td>69.289240</td>\n",
       "      <td>7.100310</td>\n",
       "      <td>178.15239</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1000.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2023</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">11</th>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.268995</td>\n",
       "      <td>82.063136</td>\n",
       "      <td>17.965620</td>\n",
       "      <td>202.643658</td>\n",
       "      <td>33.016667</td>\n",
       "      <td>1019.379167</td>\n",
       "      <td>12.380245</td>\n",
       "      <td>93.924965</td>\n",
       "      <td>22.148046</td>\n",
       "      <td>225.65854</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1021.9</td>\n",
       "      <td>3.760245</td>\n",
       "      <td>60.761055</td>\n",
       "      <td>13.104198</td>\n",
       "      <td>172.87498</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1014.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.2</td>\n",
       "      <td>7.729829</td>\n",
       "      <td>82.235617</td>\n",
       "      <td>25.143419</td>\n",
       "      <td>196.132513</td>\n",
       "      <td>69.020834</td>\n",
       "      <td>1010.795833</td>\n",
       "      <td>10.380245</td>\n",
       "      <td>91.075330</td>\n",
       "      <td>31.259941</td>\n",
       "      <td>216.73283</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1014.6</td>\n",
       "      <td>3.690246</td>\n",
       "      <td>74.427025</td>\n",
       "      <td>18.014393</td>\n",
       "      <td>177.70940</td>\n",
       "      <td>26.700000</td>\n",
       "      <td>1007.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9.3</td>\n",
       "      <td>9.101078</td>\n",
       "      <td>76.940065</td>\n",
       "      <td>27.342357</td>\n",
       "      <td>228.518643</td>\n",
       "      <td>61.875000</td>\n",
       "      <td>1005.745833</td>\n",
       "      <td>10.670245</td>\n",
       "      <td>91.051674</td>\n",
       "      <td>36.932910</td>\n",
       "      <td>244.13365</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1006.9</td>\n",
       "      <td>7.960245</td>\n",
       "      <td>53.971973</td>\n",
       "      <td>23.188583</td>\n",
       "      <td>206.56505</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1004.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.2</td>\n",
       "      <td>7.350245</td>\n",
       "      <td>82.186650</td>\n",
       "      <td>22.030794</td>\n",
       "      <td>242.243293</td>\n",
       "      <td>49.875000</td>\n",
       "      <td>1010.391667</td>\n",
       "      <td>8.660245</td>\n",
       "      <td>93.756410</td>\n",
       "      <td>38.017090</td>\n",
       "      <td>261.06042</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>4.260245</td>\n",
       "      <td>73.603580</td>\n",
       "      <td>13.905509</td>\n",
       "      <td>201.25050</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1004.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.0</td>\n",
       "      <td>5.818162</td>\n",
       "      <td>93.582846</td>\n",
       "      <td>18.648649</td>\n",
       "      <td>181.010854</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>1010.112500</td>\n",
       "      <td>11.230246</td>\n",
       "      <td>97.743355</td>\n",
       "      <td>38.821620</td>\n",
       "      <td>246.72710</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1012.9</td>\n",
       "      <td>2.140245</td>\n",
       "      <td>85.583810</td>\n",
       "      <td>11.486200</td>\n",
       "      <td>130.23636</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1008.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3603 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PT     TMEAN     RHMEAN     WSMEAN      WDMEAN    CCTMEAN  \\\n",
       "Year Month Day                                                                \n",
       "2014 1     1     1.4  4.494412  90.203694  16.615975  166.495278  59.916667   \n",
       "           2     6.8  5.978995  92.044333  20.621631  184.099628  63.875000   \n",
       "           3     0.8  6.586079  88.778159  22.263927  183.268500  50.108334   \n",
       "           4    16.5  6.358579  94.172092  15.272616  158.277724  66.666667   \n",
       "           5     5.5  4.995662  86.622807  16.897822  225.090521  59.383333   \n",
       "...              ...       ...        ...        ...         ...        ...   \n",
       "2023 11    8     0.0  7.268995  82.063136  17.965620  202.643658  33.016667   \n",
       "           9     4.2  7.729829  82.235617  25.143419  196.132513  69.020834   \n",
       "           10    9.3  9.101078  76.940065  27.342357  228.518643  61.875000   \n",
       "           11    3.2  7.350245  82.186650  22.030794  242.243293  49.875000   \n",
       "           12   14.0  5.818162  93.582846  18.648649  181.010854  85.333333   \n",
       "\n",
       "                    MSLMEAN       TMAX      RHMAX      WSMAX      WDMAX  \\\n",
       "Year Month Day                                                            \n",
       "2014 1     1    1014.029167   8.720245  96.163430  18.455048  202.01129   \n",
       "           2    1008.670833   9.240245  97.294250  29.215502  223.78113   \n",
       "           3    1013.070833  10.400246  97.533720  29.142216  210.57922   \n",
       "           4    1007.625000   8.480246  98.024340  28.100763  205.51387   \n",
       "           5    1010.754167   7.480245  97.288110  27.103000  253.00919   \n",
       "...                     ...        ...        ...        ...        ...   \n",
       "2023 11    8    1019.379167  12.380245  93.924965  22.148046  225.65854   \n",
       "           9    1010.795833  10.380245  91.075330  31.259941  216.73283   \n",
       "           10   1005.745833  10.670245  91.051674  36.932910  244.13365   \n",
       "           11   1010.391667   8.660245  93.756410  38.017090  261.06042   \n",
       "           12   1010.112500  11.230246  97.743355  38.821620  246.72710   \n",
       "\n",
       "                CCTMAX  MSLMAX      TMIN      RHMIN      WSMIN      WDMIN  \\\n",
       "Year Month Day                                                              \n",
       "2014 1     1      99.0  1017.0  0.310245  77.926410  13.584932  144.46231   \n",
       "           2     100.0  1012.1  3.530245  83.382390  14.759999  152.35402   \n",
       "           3     100.0  1016.1  2.340245  79.372375  15.716793  157.79652   \n",
       "           4     100.0  1016.3  4.600245  88.295070   7.244860  135.00000   \n",
       "           5     100.0  1016.2  0.440245  69.289240   7.100310  178.15239   \n",
       "...                ...     ...       ...        ...        ...        ...   \n",
       "2023 11    8      79.0  1021.9  3.760245  60.761055  13.104198  172.87498   \n",
       "           9     100.0  1014.6  3.690246  74.427025  18.014393  177.70940   \n",
       "           10    100.0  1006.9  7.960245  53.971973  23.188583  206.56505   \n",
       "           11     78.0  1013.0  4.260245  73.603580  13.905509  201.25050   \n",
       "           12    100.0  1012.9  2.140245  85.583810  11.486200  130.23636   \n",
       "\n",
       "                   CCTMIN  MSLMIN  \n",
       "Year Month Day                     \n",
       "2014 1     1    30.000002  1009.0  \n",
       "           2     6.000000  1005.9  \n",
       "           3    14.000000  1011.4  \n",
       "           4     5.000000   999.5  \n",
       "           5    10.800000  1000.1  \n",
       "...                   ...     ...  \n",
       "2023 11    8     5.000000  1014.8  \n",
       "           9    26.700000  1007.6  \n",
       "           10   24.000000  1004.4  \n",
       "           11   17.000000  1004.7  \n",
       "           12   14.000000  1008.1  \n",
       "\n",
       "[3603 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_data = change_resolution_to_daily(data)\n",
    "daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have $18$ _features_ and $3603$ _samples_. $\\text{PT}$ is actually the target value.\\\n",
    "For saving time, we write daily data in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_daily_data(daily_data)\n",
    "_, _, X, y = read_daily_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data\n",
    "We split our data into *train* and *test* sets with relative size $70/30$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2522, 18) (1081, 18) (2522,) (1081,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize parameters distribution\n",
    "A [histogram](https://www.investopedia.com/terms/h/histogram.asp#:~:text=A%20histogram%20is%20a%20graph,how%20often%20that%20variable%20appears) is a graph that shows the frequency of numerical data using rectangles. The height of each rectangle represents the distribution frequency of a variable. The width of the rectangle represents the value of the variable. Here is the histogram for the first 6 features and target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_histogram(X_train.values, titles=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "In general, many learning algorithms such as linear models benefit from standardization of the data set. Since some features like $\\text{MEANMSL}$ are numerically larger than others, we should sacle features. \n",
    "\n",
    "\n",
    "We standardize features by removing the mean and scaling to unit variance.\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "We find parameters of nomalization only by having *training set* and **do not** recalculate them on the *test set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "For precipitation forecating, we have two options:\n",
    "- Quantitative Precipitation Forecast (QPF) that foreacst the amount of precipitation.\n",
    "- Predict whether it will rain or not.\n",
    "### Regression\n",
    "We use $R^2$ technique for measuring accuracy. If $\\hat{y_i}$ is the predicted value for $i$-th sample:\n",
    "$$ R^2 (y, \\hat{y}) = 1 - \\frac{\\sigma{1}{n} (y_i - \\hat{y_i})^2}{\\sigma{1}{n} (y_i - \\bar{y_i})^2} $$\n",
    "#### Linear Model\n",
    "In this model, target value is expected to be a linear combination of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [Stochastic Gradient Descent](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) to train our linear model. First we find the best loss function between functions below:\n",
    "- *Huber*: less sensitive to outliers than least-squares.\n",
    "- *SquaredError*:\n",
    "$$ L(y_i, f(x_i)) = \\frac{1}{2} (y_i - f(x_i)) ^ 2 $$\n",
    "- *Epsilon-Insensitive*: equivalent to Support Vector Regression.\n",
    "- *SquaredEpsilon-Insensitive*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg_loss = SGDRegressor(random_state=42)\n",
    "sgd_loss_grid = {\"loss\": [\"huber\", \"squared_error\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]}\n",
    "sgd_loss_search = GridSearchCV(sgd_reg_loss, sgd_loss_grid, scoring=\"r2\")\n",
    "sgd_loss_model = make_pipeline(std, sgd_loss_search).fit(X_train, y_train)\n",
    "pd.DataFrame(sgd_loss_search.cv_results_, columns=[\"mean_test_score\", \"rank_test_score\", \"params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, *SquaredError* is the best choice in our situation.\n",
    "\n",
    "Next step is **Tuning the hyper-parameters**. We only tune $\\text{alpha}$ (regularization term) and $\\text{max\\_iter}$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_grid = {\n",
    "    \"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"max_iter\": [10, 100, 500, 1000]\n",
    "}\n",
    "sgd_reg = sgd_loss_search.best_estimator_\n",
    "sgd_search = GridSearchCV(sgd_reg, sgd_grid, scoring=\"r2\")\n",
    "sgd_model = make_pipeline(std, sgd_search).fit(X_train, y_train)\n",
    "pd.DataFrame(sgd_search.cv_results_, columns=[\"mean_test_score\", \"rank_test_score\", \"params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we found that $\\text{alpha} = 0.001$ and $\\text{max\\_iter} = 100$ is the best possible choice, we start predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = make_pipeline(std, sgd_search.best_estimator_).fit(X_train, y_train)\n",
    "y_train_pred = linear_model.predict(X_train, y_train)\n",
    "y_test_pred = linear_model.predict(X_test, y_test)\n",
    "linear_score = pd.DataFrame(\n",
    "    [r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)],\n",
    "    columns=[\"Train Score\", \"Test Score\"], index=[\"Linear\"]\n",
    ")\n",
    "linear_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. This is one of the models that is **not sensitive to the scale** of the input features.\n",
    "\n",
    "We tune only on $\\text{n\\_estimators}$ and $\\text{max\\_depth}$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_grid = {\n",
    "    \"n_estimators\": [10, 60, 100, 500],\n",
    "    \"max_depth\": [3, 5, 10, None]\n",
    "}\n",
    "rfr = RandomForestRegressor(random_state=42)\n",
    "rfr_search = GridSearchCV(rfr, rfr_grid, scoring=\"r2\").fit(X_train, y_train)\n",
    "pd.DataFrame(rfr_search.cv_results_, columns=[\"mean_test_score\", \"rank_test_score\", \"params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we found that $\\text{n\\_estimators} = 100$ and $\\text{max\\_depth} = 100$ is the best possible choice, we start predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_model = rfr_search.best_estimator_.fit(X_train, y_train)\n",
    "y_train_pred = rfr_model.predict(X_train, y_train)\n",
    "y_test_pred = rfr_model.predict(X_test, y_test)\n",
    "rfr_score = pd.DataFrame(\n",
    "    [r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)],\n",
    "    columns=[\"Train Score\", \"Test Score\"], index=[\"RandomForest\"]\n",
    ")\n",
    "rfr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "Another ensemble model.\n",
    "In each stage a regression tree is fit on the negative gradient of the given loss function.\n",
    "\n",
    "Here, we tune on $\\text{n\\_estimators}$, $\\text{max\\_depth}$, and also $\\text{learning\\_rate}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grb_grid = {\n",
    "    \"n_estimators\": [10, 60, 100, 500],\n",
    "    \"max_depth\": [3, 5, 10, None],\n",
    "    \"learning_rate\": [0.01, 0.1, 1]\n",
    "}\n",
    "grb = GradientBoostingRegressor(random_state=42)\n",
    "grb_search = GridSearchCV(grb, grb_grid, scoring=\"r2\").fit(X_train, y_train)\n",
    "pd.DataFrame(grb_search.cv_results_, columns=[\"mean_test_score\", \"rank_test_score\", \"params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction we use this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grb_model = grb_search.best_estimator_.fit(X_train, y_train)\n",
    "y_train_pred = grb_model.predict(X_train, y_train)\n",
    "y_test_pred = grb_model.predict(X_test, y_test)\n",
    "grb_score = pd.DataFrame(\n",
    "    [r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)],\n",
    "    columns=[\"Train Score\", \"Test Score\"], index=[\"RandomForest\"]\n",
    ")\n",
    "grb_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nueral Networks\n",
    "We use `keras` for implementing nueral networks. Here are some structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model1 = Sequential([\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    Dense(5, activation=\"relu\"),\n",
    "    Dense(1, activation=\"relu\")\n",
    "], name=\"nn_model1\")\n",
    "\n",
    "nn_model2 = Sequential([\n",
    "    Dense(15, activation=\"relu\"),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    Dense(5, activation=\"relu\"),\n",
    "    Dense(1, activation=\"relu\")\n",
    "], name=\"nn_model2\")\n",
    "\n",
    "nn_model3 = Sequential([\n",
    "    Dense(8, activation=\"relu\"),\n",
    "    Dense(1, activation=\"relu\")\n",
    "], name=\"nn_model3\")\n",
    "\n",
    "data = []\n",
    "\n",
    "for nn_model_keras in [nn_model1, nn_model2, nn_model3]:\n",
    "    nn_model_keras.compile(optimizer=Adam(learning_rate=1e-3), loss=MeanSquaredError())\n",
    "    nn_model = KerasRegressor(build_fn=nn_model_keras, verbose=0)\n",
    "    pipeline = make_pipeline(std, nn_model).fit(X_train, y_train)\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    data.append([[r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)]])\n",
    "\n",
    "nn_score = pd.DataFrame(\n",
    "    data, columns=[\"Train Score\", \"Test Score\"],\n",
    "    index=[\"NN1\", \"NN2\", \"NN3\"]\n",
    ")\n",
    "nn_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "For Regression part this is the summary of models that are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_scores = pd.concat([linear_score, rfr_score, grb_score, nn_score.iloc[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Useful links\n",
    "These links are used in mentioned parts:\n",
    "- [Normalization](https://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split)\n",
    "- [Classification](https://stackoverflow.com/questions/77607029/use-logisticregression-to-predict-precipitation-in-sklearn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
